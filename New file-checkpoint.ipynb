{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9172cad4-80a9-4ec0-a49b-2e4c68f886ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns: ['name', 'address', 'rating', 'user_ratings_total', 'place_id', 'website', 'instagram', 'email', 'location']\n",
      "\n",
      "Filtered DataFrame:\n",
      "                      name  reviews                             website  \\\n",
      "0           We Style Homes     26.0       https://www.westylehomes.com/   \n",
      "1    My Design & Build Ltd     26.0  http://www.mydesignandbuild.co.uk/   \n",
      "2      Sidorova Design Ltd     20.0      http://www.sidorovadesign.com/   \n",
      "3     Decorators Style Ltd     24.0   http://www.decoratorsstyle.co.uk/   \n",
      "4  S&B SUPERIOR DESIGN LTD      NaN                                 NaN   \n",
      "\n",
      "                         email  \n",
      "0   interiors@weatylehomes.com  \n",
      "1  info@mydesignandbuild.co.uk  \n",
      "2                          NaN  \n",
      "3  styledecorators@yahoo.co.uk  \n",
      "4                          NaN  \n",
      "\n",
      "Cleaned data saved to 'cleaned_data.csv'\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Load the CSV file\n",
    "# # Replace 'your_file.csv' with your actual file path\n",
    "# df = pd.read_csv('uk_interior_designers_set_1.csv')\n",
    "\n",
    "# # Print the column names to see what's available\n",
    "# print(\"Available columns:\", df.columns.tolist())\n",
    "\n",
    "# # Extract only the columns containing email, name, and website\n",
    "# # This will look for columns with these exact names (case-insensitive)\n",
    "# new_df = pd.DataFrame()\n",
    "\n",
    "# for col in df.columns:\n",
    "#     col_lower = col.lower()\n",
    "#     if 'email' in col_lower:\n",
    "#         new_df['email'] = df[col]\n",
    "#     elif 'name' in col_lower:\n",
    "#         new_df['name'] = df[col]\n",
    "#     elif 'website' in col_lower or 'web site' in col_lower or 'url' in col_lower:\n",
    "#         new_df['website'] = df[col]\n",
    "#     elif 'user_ratings_total' in col_lower or 'feedback' in col_lower or 'comment' in col_lower:\n",
    "#         new_df['reviews'] = df[col]\n",
    "\n",
    "# # Display the first few rows of the filtered dataframe\n",
    "# print(\"\\nFiltered DataFrame:\")\n",
    "# print(new_df.head())\n",
    "\n",
    "# # Save the filtered dataframe to a new CSV file\n",
    "# new_df.to_csv('data2.csv', index=False)\n",
    "# print(\"\\nCleaned data saved to 'cleaned_data.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2cb82e8e-6a1f-4a61-840c-654a5398c7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records in cleaned data: 507\n",
      "Records with website: 431\n",
      "Records without website: 76\n",
      "\n",
      "Data has been successfully split into:\n",
      "- 'contacts_with_website.csv'\n",
      "- 'contacts_without_website.csv'\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Load the cleaned CSV file\n",
    "# cleaned_df = pd.read_csv('data2.csv')\n",
    "\n",
    "# # Print basic information about the data\n",
    "# print(f\"Total records in cleaned data: {len(cleaned_df)}\")\n",
    "\n",
    "# # Check if the website column exists\n",
    "# if 'website' not in cleaned_df.columns:\n",
    "#     print(\"Error: 'website' column not found in the cleaned data.\")\n",
    "# else:\n",
    "#     # Create two separate dataframes\n",
    "#     # 1. Records with website values (not empty and not NaN)\n",
    "#     with_website = cleaned_df.dropna(subset=['website'])\n",
    "    \n",
    "#     # Further filter out empty strings or whitespace-only strings\n",
    "#     with_website = with_website[with_website['website'].str.strip() != '']\n",
    "    \n",
    "#     # 2. Records without website values (empty or NaN)\n",
    "#     without_website = cleaned_df[cleaned_df['website'].isna() | \n",
    "#                                (cleaned_df['website'].str.strip() == '')]\n",
    "    \n",
    "#     # Print summary of the split\n",
    "#     print(f\"Records with website: {len(with_website)}\")\n",
    "#     print(f\"Records without website: {len(without_website)}\")\n",
    "    \n",
    "#     # Save to separate CSV files\n",
    "#     with_website.to_csv('interior_designers_with_website.csv', index=False)\n",
    "#     without_website.to_csv('interior_designers_without_website.csv', index=False)\n",
    "    \n",
    "#     print(\"\\nData has been successfully split into:\")\n",
    "#     print(\"- 'contacts_with_website.csv'\")\n",
    "#     print(\"- 'contacts_without_website.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53800d9-5a38-4d6a-855b-0eeafd52e380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "66a6a915-404b-4685-8cde-06d6d0d784df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns: Index(['name', 'reviews', 'website', 'email'], dtype='object')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can only use .str accessor with string values!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m email_column \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124memail\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Agencies with email\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m with_email \u001b[38;5;241m=\u001b[39m df[df[email_column]\u001b[38;5;241m.\u001b[39mnotna() \u001b[38;5;241m&\u001b[39m (df[email_column]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Agencies without email\u001b[39;00m\n\u001b[0;32m     16\u001b[0m without_email \u001b[38;5;241m=\u001b[39m df[df[email_column]\u001b[38;5;241m.\u001b[39misna() \u001b[38;5;241m|\u001b[39m (df[email_column]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n",
      "File \u001b[1;32m~\\Anaconda\\Lib\\site-packages\\pandas\\core\\generic.py:6299\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   6293\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[0;32m   6294\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[0;32m   6295\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[0;32m   6296\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   6297\u001b[0m ):\n\u001b[0;32m   6298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 6299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name)\n",
      "File \u001b[1;32m~\\Anaconda\\Lib\\site-packages\\pandas\\core\\accessor.py:224\u001b[0m, in \u001b[0;36mCachedAccessor.__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessor\n\u001b[1;32m--> 224\u001b[0m accessor_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessor(obj)\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;66;03m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;66;03m# NDFrame\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, accessor_obj)\n",
      "File \u001b[1;32m~\\Anaconda\\Lib\\site-packages\\pandas\\core\\strings\\accessor.py:191\u001b[0m, in \u001b[0;36mStringMethods.__init__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstring_\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StringDtype\n\u001b[1;32m--> 191\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate(data)\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_categorical \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(data\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype)\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(data\u001b[38;5;241m.\u001b[39mdtype, StringDtype)\n",
      "File \u001b[1;32m~\\Anaconda\\Lib\\site-packages\\pandas\\core\\strings\\accessor.py:245\u001b[0m, in \u001b[0;36mStringMethods._validate\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    242\u001b[0m inferred_dtype \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39minfer_dtype(values, skipna\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inferred_dtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m allowed_types:\n\u001b[1;32m--> 245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only use .str accessor with string values!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inferred_dtype\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can only use .str accessor with string values!"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your CSV file\n",
    "df = pd.read_csv(\"interior_designers_without_website.csv\")  # Replace with your actual file name\n",
    "\n",
    "# Check column names\n",
    "print(\"Available columns:\", df.columns)\n",
    "\n",
    "# Replace 'email' with the actual column name if needed\n",
    "email_column = 'email'\n",
    "\n",
    "# Agencies with email\n",
    "with_email = df[df[email_column].notna() & (df[email_column].str.strip() != '')]\n",
    "\n",
    "# Agencies without email\n",
    "without_email = df[df[email_column].isna() | (df[email_column].str.strip() == '')]\n",
    "\n",
    "# Save to separate CSV files\n",
    "with_email.to_csv(\"agencies_without_emails.csv\", index=False)\n",
    "without_email.to_csv(\"agencies_without_emails2.csv\", index=False)\n",
    "\n",
    "print(\"Files saved:\")\n",
    "print(\"- agencies_without_emails.csv\")\n",
    "print(\"- agencies_without_emails2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cd1fd183-7084-4cf6-9897-5d37917726b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load both CSV files\n",
    "df1 = pd.read_csv(\"agencies_without_emails.csv\")\n",
    "df2 = pd.read_csv(\"agencies_without_emails2.csv\")\n",
    "\n",
    "# Combine (stack) the two datasets\n",
    "merged_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Save the result\n",
    "merged_df.to_csv(\"merged_output2.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "128a6c73-80e0-447a-b640-4f6277610184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping http://decoratorsinbexley.co.uk/: HTTPConnectionPool(host='decoratorsinbexley.co.uk', port=80): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x000001FE32380050>: Failed to resolve 'decoratorsinbexley.co.uk' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Error scraping http://www.steadinteriors.com/: HTTPConnectionPool(host='www.steadinteriors.com', port=80): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x000001FE327A9FA0>: Failed to resolve 'www.steadinteriors.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Error scraping http://interiordesigncambridge.co.uk/: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "Error scraping http://savinsoninteriors.com/: HTTPConnectionPool(host='savinsoninteriors.com', port=80): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x000001FE32AF9940>: Failed to resolve 'savinsoninteriors.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Error scraping http://designcld.co.uk/: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "Error scraping https://shapeandspace.co.uk/: HTTPSConnectionPool(host='shapeandspace.co.uk', port=443): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001FE325CC740>, 'Connection to shapeandspace.co.uk timed out. (connect timeout=10)'))\n",
      "Error scraping http://www.endinteriors.com/: HTTPSConnectionPool(host='endinteriors.com', port=443): Read timed out. (read timeout=10)\n",
      "Error scraping http://decoratorsinbexley.co.uk/: HTTPConnectionPool(host='decoratorsinbexley.co.uk', port=80): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x000001FE31FE73E0>: Failed to resolve 'decoratorsinbexley.co.uk' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Error scraping http://interiordesigncambridge.co.uk/: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "Error scraping http://savinsoninteriors.com/: HTTPConnectionPool(host='savinsoninteriors.com', port=80): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x000001FE322084A0>: Failed to resolve 'savinsoninteriors.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Error scraping https://juliettesinteriors.co.uk/: HTTPSConnectionPool(host='juliettesinteriors.co.uk', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001FE33101D00>: Failed to resolve 'juliettesinteriors.co.uk' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Error scraping http://designcld.co.uk/: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "Error scraping https://shapeandspace.co.uk/: HTTPSConnectionPool(host='shapeandspace.co.uk', port=443): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001FE32D51F10>, 'Connection to shapeandspace.co.uk timed out. (connect timeout=10)'))\n",
      "Error scraping http://www.endinteriors.com/: HTTPSConnectionPool(host='endinteriors.com', port=443): Read timed out. (read timeout=10)\n",
      "✅ Scraping done. File saved: design_agencies_with_scraped_emails.csv\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 's' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdesign_agencies_with_scraped_emails.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Scraping done. File saved: design_agencies_with_scraped_emails.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m s\n",
      "\u001b[1;31mNameError\u001b[0m: name 's' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"merged_output2.csv\")\n",
    "\n",
    "# Ensure the column is named 'website'\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "# A function to scrape email from a website\n",
    "def scrape_email(url):\n",
    "    try:\n",
    "        if not url.startswith(\"http\"):\n",
    "            url = \"http://\" + url\n",
    "        response = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        emails = set(re.findall(r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\", soup.get_text()))\n",
    "        return next(iter(emails), \"\")  # return first email if found\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Apply the scraping function only where email is missing\n",
    "df[\"email\"] = df.apply(lambda row: scrape_email(row[\"website\"]) if pd.isna(row.get(\"email\", None)) or row[\"email\"].strip() == \"\" else row[\"email\"], axis=1)\n",
    "\n",
    "# Save the updated file\n",
    "df.to_csv(\"design_agencies_with_scraped_emails.csv\", index=False)\n",
    "\n",
    "print(\"Scraping done. File saved: design_agencies_with_scraped_emails.csv\")\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7f033b-2108-4a1e-8488-2606c80ec85a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
